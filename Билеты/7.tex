Как непосредственно следует из названия, квазиньютоновские методы пытаются вести себя как обычный метод Ньютона. Вспомним, что на каждой итерации метода Ньютона мы решаем систему линейных уравнений (по сути обращаем гессиан), что приводит к сложности итерации $\mathcal{O}(n^3)$. Кроме того, от нас требуется подсчитывать сам гессиан, что может быть нетривиальной задачей для некоторых функций, например, для нейросетей или для логистической регрессии (привет, ТИ!). Все квазиньютоновские методы оперируют градиентом и с помощью него итеративно приближают гессиан функции. Мотивация использовать квазиньютоновские методы очень простая: мы снижаем затраты по времени, но при этом сохраняем суперлинейную скорость сходимости.

Разберем подход, который приводит к квазиньютоновским методам. Вспоминаем про многомерный ряд Тейлора и приближаем нашу гладкую функцию квадратичной моделью:

$$
f(x_k + d_k) \approx \widehat{f}_k(d_k) = f_k + \nabla f_k^T d_k + \frac{1}{2} d_k^T B_k d_k
$$

\noindent
Матрица $B_k$ - симметричная (таков гессиан) и положительно-определенная (это необходимо для выпуклости квадратичной модели $\widehat{f}_k(d_k)$, чтобы функция обладала единственным минимумом). В ванильном методе Ньютона мы используем $B_k = \nabla^2 f(x_k)$, но мы уже договорились, что такой подход нас не устраивает.

Мы хотим найти оптимальное направление, вдоль которого нужно сделать шаг. Приравняв к нулю градиент $\widehat{f}_k(d_k)$, мы получим систему линейных уравнений:

$$
B_k d_k = -\nabla f_k \Leftrightarrow d_k = -B_k \nabla f_k
$$

\noindent
После этого с помощью линейного поиска мы находим длину $\alpha_k$ и делаем шаг спуска:

$$
x_{k + 1} = x_k + \alpha_k d_k
$$

\noindent
Получаем аппроксимацию функции в новой точке:
$$
\widehat{f}_{k + 1}(d) = f_{k+1} + \nabla f_{k + 1}^T d + \frac{1}{2} d^T B_{k + 1} d
$$

Нужно сочинить какие-то разумные требования для матрицы $B_{k + 1}$, чтобы она была похожа на гессиан. Закажем, чтобы градиент функция $\widehat{f}_{k+1}$ совпадал с градиентом исходной функции $f$ в точках $x_k$ и $x_{k+1}$:

$$
\nabla f_k = \nabla f (x_{k+1} - \alpha_k d_k) = \nabla \widehat{f}_{k + 1}(-\alpha_k d_k) = \nabla f_{k+1} - \alpha_k B_{k+1} d_k
$$

$$
\nabla f_{k+1} = \nabla \widehat{f}_{k + 1}(0) = \nabla f_{k+1}
$$

\noindent
Второе уравнение тривиально, а вот из первого можно вывести линейное ограничение на $B_{k+1}$:

$$
B_{k+1} \alpha_k d_k = \nabla f_{k+1} - \nabla f_k
$$

Приняв $s_k = x_{k+1} - x_k = \alpha_k d_k$ и $y_k = \nabla f_{k+1} - \nabla f_k$, получаем ограничение, именуемое \textbf{уравнением секущей}:

$$
B_{k+1} s_k = y_k
$$

\noindent
Это ограничение используется во всех квазиньютоновских методах. При этом, искомая матрица $B_{k+1}$ существует только при условии $s_k^T y_k > 0$. Если функция $f$ сильно выпукла, то это неравенство выполнено для любых векторов $x \ne y$ (пользуемся дифференциальным критерием первого порядка):

$$
\begin{cases}
    f(y) \ge f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \|y - x\|^2 \\
    f(x) \ge f(y) + \nabla f(y)^T (x - y) + \frac{\mu}{2} \|y - x\|^2
\end{cases} \Rightarrow
$$

$$
f(x) + f(y) \ge f(x) + f(y) + \big(\nabla f(x) - \nabla f(y)\big)^T (y - x) + \mu \|y - x\|^2
$$

$$
\big(\nabla f(y) - \nabla f(x)\big)^T (y - x) \ge \mu \|y - x\|^2 > 0
$$

\noindent
Если же функция не является сильно выпуклой, то выполнение неравенство обеспечит (слабое) условие Вульфа, поэтому именно его применяют для линейного поиска в квазиньютоновских алгоритмах.

$$
\nabla f(x_{k+1})^T d_k \ge c_2 \nabla f(x_k) d_k
$$

$$
\big(\nabla f(x_{k+1}) - \nabla f(x_k)\big)^T \alpha_k d_k \ge (c_2 - 1) \alpha_k  \nabla f(x_k) d_k
$$

$$
\big(\nabla f(x_{k+1}) - \nabla f(x_k)\big)^T (x_{k+1} - x_k) \ge (c_2 - 1) \alpha_k \nabla f(x_k) d_k = (< 0) \cdot (> 0) \cdot (< 0) > 0
$$

Все квазиньютоновские методы обновляют матрицу $B_k$ по правилу $B_{k+1} = B_k + U_k$, где $U_k$ - матрица невысокого ранга. Это необходимо для эффективного вычисления обратной матрицы $H_{k + 1} = B_{k+1}^{-1} = \left(B_k + U_k \right)^{-1}$. Конкретный вид матрицы $U_k$ и отличает разные квазиньютоновские методы.

\vspace{6pt}
\textbf{\large SR-1}

SR-1 - это простейший квазиньютоновский метод, назван так за то, что обновляет гессиан симметричной матрицей ранга 1. Формула обновления $B_k$ такая:

$$
B_{k+1} = B_k + \sigma v v^T
$$

\noindent
Здесь $\sigma = +1$ или $-1$, а $\sigma$ и $v$ подбираются так, чтобы выполнялось уравнение секущей. Итоговые формулы для метода такие:

$$
B_{k+1} = B_k + \frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}
$$

$$
H_{k+1} = H_k + \frac{(s_k - H_k y_k)(s_k - H_k y_k)^T}{(s_k - H_k y_k)^T y_k}
$$

\noindent
У схемы SR-1 есть ряд недостатков: получаемые матрицы могут не быть положительно определенными, а знаменатель порой оказывается очень маленьким, что выражается в численной неусточивойсти. Однако, утверждается, что матрицы в SR-1 хорошо аппроксимируют настоящий гессиан. Со скоростью сходимости здесь сложно. Если методу удается избежать проблем с положительной определенностью и знаменателем, то сходимость суперлинейная. Но зачастую метод ведет себя очень неустойчиво.

\vspace{6pt}
\textbf{\large BFGS}

Матрица $H_{k+1}$, используемая в методе BFGS, является решением следующей оптимизационной задачи (не уверен, что это нужно, но просто забавный факт):

$$
\begin{cases}
    \|H - H_k\|_W \rightarrow \displaystyle\min_{H \in \Sym^n_{++}} \\
    H y_k = s_k
\end{cases}
$$

\noindent
Здесь используется взвешенная норма Фробениуса: $\|A\|_W = \|W^{1/2} A W^{1/2}\|_F$, а в качестве матрицы весов берется любая, удовлетворяющая условию $W s_k = y_k$. Ограничение в системе - это уравнение секущей, записанное через матрицу $H_{k+1}$ $(B_{k+1} s_k = y_k \Leftrightarrow H_{k+1} y_k = s_k)$.

Эта оптимизационная задача имеет единственное решение, которое дает нам формулу для обновления матрицы $H_k$:

$$
H_{k+1} = \left(I_n - \frac{s_k y_k^T}{\langle y_k, s_k \rangle}\right) H_k \left(I_n - \frac{y_k s_k^T}{\langle y_k, s_k \rangle}\right) + \frac{s_k s_k^T}{\langle y_k, s_k \rangle}
$$

\noindent
При этом формула для обновления $B_k$ выглядит так (она не нужна в реализации алгоритма, но ее стоит знать):

$$
B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{\langle B_k s_k, s_k \rangle} + \frac{y_k y_k^T}{\langle y_k, s_k \rangle}
$$

С первого взгляда может показаться, что мы не облегчили себе задачу: ведь при пересчете $H_k$ мы перемножаем матрицы, а сложность этой операции все так же $\mathcal{O}(n^3)$. Но если раскрыть скобки в формуле, то все операции в ней упростяться до сложности $\mathcal{O}(n^2)$ (это умножение матрицы на вектор, умножение вектора-столбца на вектор-строку и скалярное произведение):

$$
H_{k+1} = H_k + \frac{\left(y_k^T s_k + y_k^T H_k y_k\right) s_k s_k^T}{\langle y_k, s_k \rangle^2} - \frac{s_k y_k^T H_k + H_k y_k s_k^T}{\langle y_k, s_k \rangle}
$$

Осталось договориться о выборе начального приближения $H_0$. На этот случай нет универсального варианта, и обычно принимают либо $H_0 = I_n$, либо можно оценить гессиан в точке $x_0$ через конечные разности и обратить его.

Общая схема метода выглядит так:

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \Procedure{BFGS}{$f, x_0, H_0, \varepsilon$}
            \For{$k=0, 1, 2, ...$}
                \State $d_k \gets -H_k \nabla f_k$
                \State $\alpha_k \gets \textsc{LineSearch}(f, x_k, d_k)$
                \State $x_{k+1} \gets x_k + \alpha_k d_k$
                \State $s_k \gets x_{k+1} - x_k$
                \State $y_k \gets \nabla f_{k+1} - \nabla f_k$
                \State $H_{k+1} \gets \textsc{UpdateInversedHess}(H_k, s_k, y_k)$
                \If{$\|\nabla f_{k+1}\| < \varepsilon$}
                    \textbf{break}
                \EndIf
            \EndFor
            \State \Return $x_k$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

К сожалению, ничто в этом мире не идеально, даже метод BFGS. Нам удалось сократить время итерации до $\mathcal{O}(n^2)$, но нам все еще нужно хранить плотную матрицу $H_k$, и это обеспечивает затраты на память $\mathcal{O}(n^2)$, что не всегда возможно. Тем не менее, метод BFGS имеет суперлинейную сходимость, что делает его популярным на практике.

\vspace{6pt}
\textbf{\large L-BFGS}

Метод L-BFGS - это модификация стандартного BFGS, которая позволяет сэкономить память и не хранить гессиан целиком. Все что от нас требуется - уметь умножать гессиан на произвольный вектор. Для этого метод поддерживает историю $\mathcal{H}_k = \{(s_{k - i}, y_{k - i})\}_{i=1}^l$ из последних $l$ пар векторов. Вводится начальное приближение $H_{k-l}$:

$$
H_{k-l} = \gamma_0^{(k)} I_n, \text{ где } \gamma_0^{(k)} = \frac{\langle y_{k-1}, s_{k-1} \rangle}{\langle y_{k-1}, y_{k-1} \rangle}
$$

Новое приближение $H_k$ получается из $H_{k-l}$ путем $l$-кратного применения формулы из BFGS. Однако существует рекурсивная процедура, позволяющую подсчитывать произведение $d_k = -H_k \nabla f_k$ без формирования матриц в памяти. Выглядит она так:

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \Procedure{Multiply}{$v, \mathcal{H}, \gamma_0$}
            \If{$\mathcal{H}$ = \O}
                \Return $\gamma_0 v$
            \EndIf
            \State $(s, y) \gets \text{ последняя пара из } \mathcal{H}$
            \State $\mathcal{H}' \gets \mathcal{H} \text{ без последней пары}$
            \State $v' \gets v - \frac{\langle s, v \rangle}{\langle y, s \rangle} y$
            \State $z \gets \textsc{Multiply}(v', \mathcal{H}', \gamma_0)$
            \State \Return $z + \frac{\langle s, v \rangle - \langle y, z \rangle}{\langle y, s \rangle} s$
        \EndProcedure

        \State $\gamma_0^{(k)} = \frac{\langle y_{k-1}, s_{k-1} \rangle}{\langle y_{k-1}, y_{k-1} \rangle}$
        \State $d_k = -\textsc{Multiply}(\nabla f_k, \mathcal{H}_k, \gamma_0^{(k)})$
    \end{algorithmic}
\end{algorithm}

Таким образом, сложность итерации для вычисления направления $d_k$ получается $\mathcal{O}(nl)$ (без учета сложности вычисления функции и градиента), а необходимая память для поддержания истории - $\mathcal{O}(nl)$. Типичное значение параметра размера истории $l=10$ (при $k < l$ история $\mathcal{H}_k$ состоит только из $k$ пар). Заметим, что при $l=\infty$ мы в точности получаем метод BFGS, а при $l=0$ - обычный градиентный спуск. На практике L-BFGS показывает себя лучше методов оптимизации первого порядка, хоть и не обладает суперлинейной сходимостью (гарантирована только линейная скорость сходимости).
