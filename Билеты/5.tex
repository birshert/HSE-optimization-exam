Преамбула: здесь мы построим итеративный, относительно легковесный и при этом точный метод решения СЛАУ с хорошими свойствами сходимости, а потом модифицируем его для оптимизации произвольных функций.

    Начнем с рассмотрения линейного метода сопряженных градиентов (далее CG). В сущности, это метод для решения СЛАУ вида $$Ax = b$$ где $A$ - симметричная положительно определенная матрица размера $n \times n$. Данную проблему, как известно, можно эквивалентно переформулировать в виде задачи оптимизации следующим образом: $$ \phi(x) = \frac{1}{2}\langle Ax, x\rangle - \langle b, x \rangle \to \min\limits_x$$
    Поскольку в обоих формулировках задача имеет одно и то же (притом единственное) решение, мы можем интерпретировать CG как метод решения СЛАУ или же как метод оптимизации выпуклых квадратичных функций. Отметим, что невязка СЛАУ $$r(x) = Ax-b$$ эквивалентна градиенту оптимизируемой функции: $$\nabla_x \phi(x) = Ax - b$$ В частности, на $k$-ом шаге: $$r_k = Ax_k - b$$Начнем немного издалека.

   \textbf{Определение.} Будем называть набор ненулевых векторов $\{ p_0, p_1,..., p_m\}$  \textbf{сопряженным} по отношению к симметричной положительно определенной матрице $A$, если
    \begin{align}
    p_i^T A p_j = 0, \ \ \ \  \forall i \neq j
    \end{align}
    Отметим, что такой набор векторов является линейно независимым. В самом деле, пусть $$\exists \alpha: \ \ \ \sum\limits_{i=0}^{n-1} \alpha_i p_i = 0$$
    Домножим каждое слагаемое справа на $Ap_j$ - все слагаемые кроме $j$-ого обнулятся (прямо как сроки Путина). При $j$-ом слагаемом придется поставить нулевой коэффициент. Повторим $n$ раз, конец.

    Важное свойство сопряженности: мы можем минимизировать $\phi(\cdot)$  за $n$ шагов, последовательно минимизируя ее вдоль каждого направления в сопряженном наборе. Для этого рассмотрим метод \textit{сопряженных направлений} (это пока не CG, к нему придем чуть позже).

    Пусть нам данно начальное приближение $x_0 \in \mathbb{R}^n$  и набор сопряженных направлений $\{p_0, p_1,..., p_{n-1}\}$. Будем генерировать последовательность $\{x_k\}$ следующим образом: $$x_{k+1} = x_k + \alpha_k p_k$$при этом задавая $\alpha_k$  как минимум $f(\alpha) = \phi(x_k + \alpha p_k)$ - в силу квадратичности имеется closed-form выражение: $$\alpha_k = -\frac{\langle r_k, p_k \rangle}{\langle Ap_k, p_k \rangle} $$
   	 \textbf{Теорема.} Оказывается, что для любого $x_0 \in \mathbb{R}^n$ последовательность $\{ x_k\}$. сгенерированная методом сопряженных направлений, сходится к решению исходной СЛАУ $x^*$ в худшем случае за $n$ шагов.

    \textbf{Док-во.} Во-первых отметим, что Кропотов умудрился сжать это в одну строчку. Гений, не иначе.

    Поскольку набор сопряженных направлений линейно независим, он порождает пространство $\mathbb{R}^n$. Таким образом, мы можем записать разность между $x_0$ и $x^*$ следующим образом: $$x^* - x_0 = \sigma_0 p_0 + ... + \sigma_{n-1} p_{n-1}$$ для некоторых множителей $\sigma$. Домножим на $p_k^T A$ слева и используем свойство сопряженности: $$ \sigma_k = \frac{p_k^T A (x^* - x_0)}{p_k^T A p_k}$$ Покажем, что $\sigma_k$ совпадают с размерами шага $\alpha_k$. Для сгенерированного алгоритмом $x_k$ верно $$x_k = x_0 + \alpha_0 p_0 + ... + \alpha_{k-1} p_{k-1}$$ Домножим слева на $p_k^T A$ и используем свойство сопряженности: $$ p_k^T A (x_k - x_0) = 0$$ Отсюда получаем $$p_k^T A (x^* - x_0) = p_k^T A (x^* - x_k) = p_k^T (b - Ax_k) = -p_k^T r_k$$ Подставим это в выражение для $\sigma_k$ и получим, что $x^*$ является суммой $x_0$ и линейной комбинации $p_i$, что дает нам гарантию сходимости за $n$ шагов. $\square$

    Здесь стоит отметить еще одно важное свойство метода (видимо, без док-ва?): невязка $r_k$ на $k$-ом шаге ортогональна всем предыдущим направлениям поиска $p_0, ...., p_{k-1}$, то бишь $$r_k^T p_i = 0, \ \ \ \ \forall i = 0,1,...,k-1$$

    \textbf{Ближе к делу.}

    Теперь перейдем наконец к CG. Самое важное, что стоит запомнить сейчас - его можно получить из метода сопряженных направлений с помощью \textit{любого} алгоритма подбора сопряженных направлений. Здесь мы можем использовать к примеру собственные векторы или модифицировать процесс Грама-Шмидта для подбора сопряженных векторов. Тем не менее, это все вычислительно затратно. В идеале нам хотелось бы уметь пересчитывать текущее направление поиска $p_k$ с использованием \textit{только} предыдущего направления $p_{k-1}$ так, чтобы $p_k$ было автоматически сопряжено ко всем направлениям до этого ($p_0,...,p_{k-2}$).

    Собственно, будем делать это следующим образом. Возьмем $p_k$ как линейную комбинацию отрицательной невязки $-r_k$ (вспомним, что это антиградиент!), и предыдущего направления поиска $p_{k-1}$: $$p_k = -r_k + \beta_k p_{k-1}$$где $\beta_k$ определяется требованием сопряженности направлений $p_k$ и $p_{k-1}$. По факту, домножим на $p_{k-1}^T A$, вспомним про сопряженность и получим: $$\beta_k = \frac{r_k^T A p_{k-1}}{p_{k-1}^T A p_{k-1}}$$ Теперь у нас есть все, чтобы сформулировать первую версию алгоритма!

    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{CG}{$A, b, x_0$}
            \State $r_0 \gets Ax_0 - b$
            \State $p_0 \gets -r_0$
            \State $k \gets 0$
            \While{$r_k \neq 0$}
                \State $\alpha_k \gets -\frac{r_k^T p_k}{p_k^T A p_k}$
                \State $x_{k+1} \gets x_k + \alpha_k p_k$
                \State $r_{k+1} \gets Ax_{k+1} -b$
                \State $\beta_{k+1} \gets \frac{r_{k+1}^T A p_k}{p_k^T A p_k}$
                \State $p_{k+1} \gets -r_{k+1} + \beta_{k+1} p_k$
                \State $k \gets k+1$
            \EndWhile
            \State \Return $x_{k+1}$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
Однако, для использования метода на практике стоит внести некоторые коррективы. В первую очередь, формулу для обновления $\alpha_k$ можно переписать следующим образом: $$\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}$$ в силу стр. 10 алгоритма и ортогональности текущей невязки всем предыдущим направлениям поиска.

Кроме того, поскольку разность невязок $$r_{k+1} - r_k = \alpha_k A p_k$$ мы можем переформулировать обновление $\beta_k$ следующим образом: $$\beta_{k+1} = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$$ Кроме того, часто может быть излишним ждать полной сходимости - из-за численных погрешностей она может даже и не наступить - поэтому разумно использовать критерий остановки по норме невязки. Теперь попробуем записать финальную версию алгоритма:

     \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{CG-Finale}{$A, b, x_0, \epsilon$}
            \State $r_0 \gets Ax_0 - b$
            \State $p_0 \gets -r_0$
            \State $k \gets 0$
            \While{$||r_k|| > \epsilon$}
                \State $\alpha_k \gets \frac{r_k^T r_k}{p_k^T A p_k}$
                \State $x_{k+1} \gets x_k + \alpha_k p_k$
                \State $r_{k+1} \gets r_k + \alpha_k A p_k$
                \State $\beta_{k+1} \gets \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$
                \State $p_{k+1} \gets -r_{k+1} + \beta_{k+1} p_k$
                \State $k \gets k+1$
            \EndWhile
            \State \Return $x_{k+1}$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}

    Отметим, что в любой момент алгоритма нам не нужно помнить векторы $r, p, x$ более чем на две последних итерации, потому в реализации мы можем спокойно перезаписывать старые значения. CG рекомендуется использовать только для больших задач, в малых больше подойдут основанные на факторизациях решения - в силу их меньшей чувствительности к погрешностям вычислений.

    \textbf{Упражнение:} показать, что в методе сопряженных градиентов градиенты не являются сопряженными.

    Теперь поговорим о сходимости метода. Доказывать здесь, думаю, ничего не надо. Мы уже обсудили, что за $n$ итераций имеет место гарантированная сходимость. Однако, при определенных свойствах распределения собственных значений матрицы $A$ сходимость может наступить гораздо раньше.
    \begin{enumerate}
    \item Если у матрицы есть только $r$ различных собственных значений, метод сойдется к точному решению за не более чем $r$ итераций.
    \item Если у матрицы есть $r$ различимых кластеров собственных значений, метод сойдется примерно за r итераций.
    \end{enumerate}

Также имеют место следующие оценки: $$||x_{k+1} - x^*||^2 \leq \left( \frac{\lambda_{n-k} - \lambda_1}{\lambda_{n-k} + \lambda_1} \right)^2 ||x_0 - x^*||^2$$ где $\lambda_1 \leq ... \leq \lambda_n$ - собственные значения $A$.
$$||x_k - x^*|| \leq 2 \left( \frac{\sqrt{k(A)} - 1}{\sqrt{k(A)} + 1} \right)^k ||x_0 - x^*||$$
где $k(A) = \frac{\lambda_n}{\lambda_1}$ - число обусловленности задачи.

    В случае, если у задачи плохая обусловленность, мы можем попробовать использовать предобуславливание - то есть, перейти к новым переменным $$\hat{x} = Cx$$ где $C$ - невырожденная матрица, и решать новую систему $$(C^{-T} A C^{-1}) \hat{x} = C^{-T} b$$ основываясь на свойствах уже трансформированной матрицы. (стасян, привет!)

    \textbf{Нелинейная часть}

    CG можно интерпретировать, как уже говорилось, в качестве метода решения СЛАУ или же в качестве метода оптимизации квадратичной функции. Однако, оказывается, что данный метод можно использовать также для минимизации произвольных нелинейных функций $f(x)$ - потребуется только дифференцируемость (в целом даже выпуклость необязательна). Для этого нам потребуется изменить следующие вещи:
    \begin{enumerate}
\item Теперь $r_k$ будет вычисляться как честный градиент $\nabla f(x)$ в точке $x_k$.
\item $\alpha_k$ теперь не будет иметь аналитического оптимального значения, поэтому будем искать его линпоиском. При этом, чтобы получить гарантированное направление спуска, нам потребуются условия Вульфа с константой $c_2 < 0.5$ .
\end{enumerate}

Это называется \textbf{алгоритмом Флетчера-Ривса.} Известно много вариантов данного алгоритма, в первую очередь различающихся методом обновления параметра $\beta$. Важной модификацией является \textbf{метод Полака-Рибье}, в котором $\beta$ обновляется следующим образом: $$\beta_{k+1} = \frac{r_{k+1}^T (r_{k+1} - r_k)}{r_k^T r_k}$$ Однако, в общем случае такая модификация опять может портить направление $p_k$. Чтобы от этого избавиться, будем брать $ReLU(\beta)$ - тогда сильное условие Вульфа гарантирует, что $p_k$ будет направлением спуска.  На практике, с неточной процедурой линпоиска метод PR демонстрирует большую работоспособность и стабильность.
