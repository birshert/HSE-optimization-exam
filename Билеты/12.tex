\textbf{Проксимальнй градиентный метод для задачи композитной оптимизации}:\\
В субградиентном методе ранее обсуждалось, что выпуклые но негладкие функции общего вида (т.е. они не в каждой точке дифференцируемые), оптимизировать довольно сложно. Поэтому данный метод переходит от общего вида выпуклых негладких функций к конкретному \textbf{композитному виду функции}, который и оптимизирует.\\

$F(x)=f(x)+h(x)$~-~композитная функция, где $f(x)$~-~её выпуклая, непрерывная и непрерывная по 1 производной часть ($f \in C; f \in C^1$), а $h(x)$~-~выпуклая, непрерывная ($h(x) \in C$), но $h \not\in С^1$. Также функция $h(x)$~-~должна быть простой. Т.е. проксимальный оператор должен легко вычислятся (о нем ниже).\\

Для показа работы проксимального метода будем использовать пример, где $f(x)$~-~просто подходящая функция по всем трем параметрам, а $h(x)=\lambda ||x||_1$. Тогда $f(x)$~-~будет представлять из себя сепарабельную часть функции, также определим, что $f \in C^{1,1}_L$(т.е. у нее на производной есть какое-то ограниение липшица). Далее распишем $F(x)$ как
$$
F(x)=f(x)+h(x) \leq m_k(x)=f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{L}{2}||x-x_k||^2+h(x) \to \underset{x}{\min}
$$
С негладкой часть пока ничего не сделали, гладкую же расписали через ур-е касательной и канстанту липшица. Дальше выдилим отдельно все слагаемые, зависящие от $x$:
$$
m_k(x)=\frac{L}{2}(x^Tx-2x^Tx_k+\frac{2}{L}\nabla f(x_k)^Tx)+h(x)+const=\frac{L}{2}||x-(x_k-\frac{1}{L}\nabla f(x_k))||^2_2+h(x)+const \to \underset{x}{\min}
$$
$$
\frac{1}{2}||x-(x_k-\frac{1}{L}\nabla f(x_k))||^2_2+\frac{1}{L}h(x) \to \underset{x}{\min}
$$
Далее как раз введем понятие проксимального оператора:\\
$$
prox_h(x):=~\underset{y}{\argmin}\left[\frac{1}{2}||y-x||^2+h(y)\right]
$$
Тогда через этот оператор можно явно показать
$$
\underset{x}{\argmin}m_k(x)=prox_{\frac{1}{L}h}(x_k-\frac{1}{L}\nabla f(x_k))
$$
Это по факту и есть шаг проксимального градиентного метода. Дальше покажем примеры вычисления проксимального оператора:\\
1)~$h(x) \equiv 0 \to prox_h(x):=\underset{y}{\argmin} \frac{1}{2}||y-x||^2 \to x=x$\\
2)~С-вып.; $h(x)=I_c(x)=\begin{cases}
0,~x \in C\\
+\infty,~x \notin C
\end{cases}$\\
В этом примере $I_c(x)$~-~индикаторная функция множества.\\
$\underset{x}{\min}F(x)=\underset{x}{\min}(f(x)+I_c(x))\equiv \underset{x \in C}{\min}f(x)$\\
Тогда $prox_h(x)=\underset{y \in C}{\argmin}\frac{1}{2}||y-x||^2=pr_c(x)$\\
В этом примере важно то, что теория проксимального оператора может применяться не только к выпуклой и непрерывной функции $h(x)$, но и к замкнутой, как в случае с индикаторной функцией.\\
3)~$h(x)=\lambda ||x||_1$~-~наш пациент.
$$
prox_{\lambda ||*||_1}(x)=\underset{y}{\argmin}\left( \frac{1}{2}||y-x||^2+\lambda||y||_1\right)=
\left\{ \underset{y_i}{\argmin}\left(\frac{1}{2}(y_i - x_i)^2+\lambda |y_i|\right) \right\}^n_{i=1}
$$
От поиска $\argmin$ по каждой компоненте можем перейти к вычислению субдифференциала вот такой функции
$$
f(y_i)=\frac{1}{2}(y_i - x_i)^2+\lambda |y_i| \to f(y)=\frac{1}{2}||y-x||^2+\lambda||y||_1
$$
$$
\partial f(y)=y-x+\lambda \partial|y|=y-x+\begin{cases}
\lambda,~y > 0,\\
-\lambda,~y < 0,\\
[-\lambda,\lambda],y=0
\end{cases}
$$
Далее помним, что в субдифференциале от точки оптимума должен содержаться $0$, тогда\\
$0 \in \partial f(y) \to$\\
$
1)~y>0 \to 0=y-x+\lambda \to y=x-\lambda > 0 \to x > \lambda\\
2)~y<0 \to 0=y-x-\lambda \to y=x+\lambda < 0 \to x < -\lambda\\
3)~y=0 \to 0 \in y(=0) - x + [-\lambda,\lambda] = [-x - \lambda,-x+\lambda] \to \begin{cases}
-x-\lambda \leq 0,\\
-x + \lambda \geq 0
\end{cases} \to -\lambda \leq x \leq \lambda\\
prox_{\lambda ||*||_1}(x)=\begin{cases}
x-\lambda, x > \lambda,\\
x + \lambda, x < -\lambda,\\
0,~|x| \leq \lambda
\end{cases}
$\\
Далее более подробно распишем вывод критерия останова и сходимости:\\
$y=prox_{\alpha h}(x - \alpha\nabla f(x))$~-~запись поиска след точки итерации через проксимальный оператор в общем виде\\
$y=prox_{\alpha h}(x -\alpha\nabla f(x)))=x-\alpha G_\alpha(x)$~-~здесь мы поиск новый точки переписываем через некоторый шаг и такую составляющую как $G_\alpha(x)=\frac{1}{\alpha}(x-y)$~-~градиентное отображение. ЭТо сделано для того, чтобы шаг проксимального метода походил на шаг градиентного спуска, т.е. тут свой размер шага и свое направление.\\
Далее покажем, что если $y$~-~это еаша точка оптимума, то в субдифференциале от проксимального оператора в этой точке оптимума будет лежать $0$.\\
$0 \in y - (x - \alpha\nabla f(x))+\alpha\partial h(y) \to \{y-x=-\alpha G_\alpha (x)\} \to |:\alpha \\
0 \in -\alpha G_\alpha (x) + \nabla f(x) + \partial h(y) \to\\
G_\alpha (x) - \nabla f(x) \in \partial h(y)$.\\
Получаем, что такой вектор как разница между градиентным отображением и градиентном от гладкой части композит. функции будет являться субградиентом для $h(y)$. Тогда отсюда подбираемся к самому полезному утверждению по этому билету:\\
\textbf{Утверждение}.~~~$x$~-~точка оптимума композит. функции $F \Leftrightarrow G_\alpha (x)=0$.\\

В лекции у этого утверждение было док-во в обе стороны.\\
1)$~\leftarrow~G_\alpha (x)=0 \to y=x-\alpha G_\alpha(x)=x \to G_\alpha (x) - \nabla f(x) \in \partial h(x)$~(т.к. $y=x$)\\
$\to G_\alpha (x) \in \nabla f(x) + \partial h(x)=\partial F(x)$,а~$G_\alpha (x)=0 \to 0 \in \partial F(x) \to x$~-~точка оптимума $F$.\\
Отсюда получаем идею про критерий останова этого метода:$||G_\alpha (x)||^2 \leq \varepsilon$. Т.е. будем требовать определенную точность градиентного отображения с каждого шага итерации метода.\\
Теперь докажем теоремку в другую сторону. Перед этим снова явно укажем принадлежность выпуклой части композит. функции к какому-то классу: $f(x) \in C^{1,1}_L$. Тогда
$$
y=x-\alpha G_\alpha(x) \to F(y)=f(y)+h(y) \leq f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}||y-x||^2+h(y)
$$
Далее подставляем выражение для $y$:
$$
=f(x)-\alpha\nabla f(x)^T G_\alpha (x) + \frac{L\alpha^2}{2}||G_\alpha (x)||^2+h(y) \leq \{\alpha \leq \frac{1}{L}\} \leq
$$
Далее мы ограничили длину шага $\alpha$ другой величиной и получили следующее:
$$
\leq f(x) - \alpha\nabla f(x)^T G_\alpha (x)+\frac{\alpha}{2}||G_\alpha (x)||^2+h(y)
$$
Теперь сделаем оценку сверху через какую нибудь третью точк $z$, чтобы связать между собой $f(x)$ и $h(y)$:
$$
\begin{cases}
f(z) \geq f(x) + \nabla f(x)^T(z-x)+\frac{\mu}{2}||z-x||^2\\
h(z) \geq h(y) + (G_\alpha (x)-\nabla f(x))^T(z-y)
\end{cases}
$$
в выражении $f(z)$ последнее слагаемое появляется из-за сильной выпуклости у $f(x)$, $h(y)$ же мы расписываем через какой-то субградиент, это все нам дает свести наше нер-во выше к след виду:
$$
\leq f(z) + h(z)(=F(z)) - \nabla f(x)^T(z-x)-\frac{\mu}{2}||z-x||^2-\alpha \nabla f(x)^T G_\alpha (x)+\frac{\alpha}{2}||G_\alpha (x)||^2-(G_\alpha (x)-\nabla f(x))^T(z-y)(=z-x+\alpha G_\alpha (x))
$$
если раскрыть скобки в последнем слагаемом, то из выражение уйдут части, связанные с $\nabla f(x)$
$$
\to F(z) - \frac{\mu}{2}||z-x||^2-G_\alpha (x)^T(z-x)-\frac{\alpha}{2}||G_\alpha(x)||^2
$$
если за точку $z$ мы возьмем $x$. То как раз получим док-во утверждения выше в обратную сторону:\\
$$
F(y) \leq F(x) - \frac{\alpha}{2}||G_alpha (x)||^2
$$
Отсюда явно можем сказать, что проксимальный метод - метод спуска. и если $x$~-~т. минимума, то отсюда видно, что $G_\alpha (x)=0$.\\
Для поиска сходимости рассмотрим $z=X_{opt}$:
$$
F(y) \leq F_{opt} - \frac{\mu}{2}||x-x_{opt}||^2+G_\alpha (x)^T(x-x_{opt})-\frac{\alpha}{2}||G_\alpha (x)||^2=F_{opt}-\frac{\mu}{2}||x-x_{opt}||^2+\frac{1}{2\alpha}(||x-x_{opt}||^2-||x-x_{opt}-\alpha G_\alpha (x)||^2)
$$
Проделали мув с перегруппировкой норм, также используем $y=x-\alpha G_\alpha(x)$,теперь можем получить след нер-ва:
$$
0 \leq F(y) - F_{opt} \leq \frac{1}{2\alpha}(||x-x_{opt}||^2-||y-x_{opt}||^2-\alpha\mu||x-x_{opt}||^2)
$$
$$
\to (1-\alpha\mu)||x-x_{opt}||^2 - ||y-x_{opt}||^2 \geq 0 \to ||y-x_{opt}||^2 \leq (1-\alpha\mu)||x-x_{opt}||^2
$$
Т.е. фактически получили, что у прокс. метода линейная скорость сходимости с константой $1-\alpha\mu$ (сдева невязка функции на новой итерации, справа - на старой итерации).\\
Дальше нужно что-то сказать про подбор константы липшица на каждой итерации, которая будет использована в коэф-те длины шага:\\
$
x_{k+1}=prox_{\frac{1}{l}h}(x_k-\frac{1}{L}\nabla f(x))\\
\text{если~}f(x_{k+1})\leq f(x_k)+\nabla f(x_k)^T(x_{k+1}-x_k)+\frac{L}{2}||x_{k+1}-x_k||^2\text{~, то выход}\\
\text{иначе~}L \leftarrow L * \gamma: \gamma > 1\\
$
$
G_{1/L}(x_k)=L(x_k-x_{k+1})\\
\text{если~}||G_{1/L}||^2 \leq \varepsilon \text{~, то выход}\\
L \leftarrow L * \rho;~\rho < 1
$\\
В целом все. Док-ва прям точные может даже и не потрубуются, но формулку для подсчета каждой новой точки, и про сходимость и критерий останова точно что-то надо будет сказать.\\
Еще стоит сказать, что прокс. методы оптимизации это семейство методов, и вполне существует проксимальный метод Ньютона, где вместо слагаемого с конст. липшица вы пишете слагаемое с гессианом, а все остальное остается также, вплоть до подсчета проксимального оператора.